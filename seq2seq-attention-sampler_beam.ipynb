{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import Queue, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't use all the VRAM!\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.15)\n",
    "#sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# We're on CPU!\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data variables\n",
    "seq_length = 16\n",
    "out_seq_length = 16\n",
    "batch_size = 1\n",
    "vocab_size = 26 + 1               # 0 for padding\n",
    "embedding_dim = 26\n",
    "\n",
    "# Network hyperparameters\n",
    "memory_dim = 200\n",
    "num_layers = 1\n",
    "\n",
    "# Training variables\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First build input placeholders and constants. The `seq2seq` API generally deals with lists of tensors, where each tensor represents a single timestep. An input to an embedding encoder, for example, would be a list of `seq_length` tensors, each of which is of dimension `batch_size` (specifying the embedding indices to input at a particular timestep).\n",
    "\n",
    "We allocate a `labels` placeholder using the same convention. A `weights` constant specifies cross-entropy weights for each label at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_inp = [tf.placeholder(tf.int32, shape=(None,), name=\"inp%i\" % t) for t in range(seq_length)]\n",
    "labels = [tf.placeholder(tf.int32, shape=(None,), name=\"labels%i\" % t) for t in range(out_seq_length)]\n",
    "weights = [tf.ones_like(labels_t, dtype=tf.float32) for labels_t in labels]\n",
    "use_initial = tf.placeholder(tf.int32, shape=[], name=\"use_initial\")\n",
    "supplied_prev  = tf.placeholder(tf.float32, shape=(1, 28), name=\"supplied_prev\")\n",
    "supplied_state = tf.placeholder(tf.float32, shape=(1, memory_dim*2), name=\"supplied_state\")\n",
    "supplied_attns = tf.placeholder(tf.float32, shape=(1, memory_dim), name=\"supplied_attns\")\n",
    "\n",
    "# Decoder input: prepend some \"GO\" token and drop the final\n",
    "# token of the decoder output\n",
    "dec_inp = [tf.placeholder(tf.int32, shape=(None,), name=\"dec_inp%i\" % t) for t in range(out_seq_length)]\n",
    "\n",
    "# Initial memory value for recurrence.\n",
    "prev_mem = tf.zeros((batch_size, memory_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the sequence-to-sequence graph.\n",
    "\n",
    "There is a **lot** of complexity hidden in these two calls, and it's certainly worth digging into both in order to really understand how this is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fbe6f359190>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "constituent_cell = tf.nn.rnn_cell.BasicLSTMCell(memory_dim)\n",
    "\n",
    "if num_layers > 1:\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([constituent_cell] * num_layers)\n",
    "else:\n",
    "    cell = constituent_cell\n",
    "\n",
    "# Without teacher forcing, with attention\n",
    "#ntf_dec_outputs, ntf_dec_memory = tf.nn.seq2seq.embedding_attention_seq2seq(enc_inp, dec_inp, cell, vocab_size+1, vocab_size+1, embedding_dim, feed_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.ops import rnn_cell, rnn, nn_ops, math_ops, embedding_ops, array_ops\n",
    "\n",
    "linear = rnn_cell._linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _extract_argmax_and_embed(embedding, output_projection=None,\n",
    "                              update_embedding=True):\n",
    "  \"\"\"Get a loop_function that extracts the previous symbol and embeds it.\n",
    "  Args:\n",
    "    embedding: embedding tensor for symbols.\n",
    "    output_projection: None or a pair (W, B). If provided, each fed previous\n",
    "      output will first be multiplied by W and added B.\n",
    "    update_embedding: Boolean; if False, the gradients will not propagate\n",
    "      through the embeddings.\n",
    "  Returns:\n",
    "    A loop function.\n",
    "  \"\"\"\n",
    "  def loop_function(prev, _):\n",
    "    if output_projection is not None:\n",
    "      prev = nn_ops.xw_plus_b(\n",
    "          prev, output_projection[0], output_projection[1])\n",
    "    prev_symbol = math_ops.argmax(prev, 1)\n",
    "    # Note that gradients will not propagate through the second parameter of\n",
    "    # embedding_lookup.\n",
    "    emb_prev = embedding_ops.embedding_lookup(embedding, prev_symbol)\n",
    "    if not update_embedding:\n",
    "      emb_prev = array_ops.stop_gradient(emb_prev)\n",
    "    return emb_prev\n",
    "  return loop_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with variable_scope.variable_scope(\"embedding_attention_seq2seq\"):\n",
    "    num_encoder_symbols = vocab_size + 1\n",
    "    num_decoder_symbols = vocab_size + 1\n",
    "    embedding_size      = embedding_dim\n",
    "\n",
    "    # Encoder.\n",
    "    encoder_cell = rnn_cell.EmbeddingWrapper(cell, embedding_classes=num_encoder_symbols, embedding_size=embedding_size)\n",
    "    encoder_outputs, encoder_state = rnn.rnn(encoder_cell, enc_inp, dtype=dtypes.float32)\n",
    "\n",
    "    # First calculate a concatenation of encoder outputs to put attention on.\n",
    "    top_states = [array_ops.reshape(e, [-1, 1, cell.output_size]) for e in encoder_outputs]\n",
    "    attention_states = array_ops.concat(1, top_states)\n",
    "\n",
    "    # Decoder.\n",
    "    cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n",
    "        \n",
    "    with variable_scope.variable_scope(\"embedding_attention_decoder\"):\n",
    "        embedding = variable_scope.get_variable(\"embedding\", [num_decoder_symbols, embedding_size])\n",
    "        loop_function = _extract_argmax_and_embed(embedding, None, True)\n",
    "        emb_inp = [embedding_ops.embedding_lookup(embedding, i) for i in dec_inp]\n",
    "        \n",
    "        decoder_inputs = emb_inp\n",
    "        initial_state = encoder_state\n",
    "        output_size = cell.output_size\n",
    "        \n",
    "        # Attention Decoder\n",
    "        with variable_scope.variable_scope(\"attention_decoder\"):\n",
    "            batch_size_ = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n",
    "            attn_length = attention_states.get_shape()[1].value\n",
    "            attn_size = attention_states.get_shape()[2].value\n",
    "\n",
    "            # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n",
    "            hidden = array_ops.reshape(attention_states, [-1, attn_length, 1, attn_size])\n",
    "            hidden_features = []\n",
    "            v = []\n",
    "            attention_vec_size = attn_size  # Size of query vectors for attention.\n",
    "            \n",
    "            for a in xrange(1):\n",
    "                k = variable_scope.get_variable(\"AttnW_%d\" % a, [1, 1, attn_size, attention_vec_size])\n",
    "                hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n",
    "                v.append(variable_scope.get_variable(\"AttnV_%d\" % a, [attention_vec_size]))\n",
    "\n",
    "            state = tf.cond(use_initial > 0, lambda: initial_state, lambda: supplied_state)\n",
    "\n",
    "            def attention(query):\n",
    "                \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n",
    "                ds = []  # Results of attention reads will be stored here.\n",
    "                \n",
    "                for a in xrange(1):\n",
    "                    with variable_scope.variable_scope(\"Attention_%d\" % a):\n",
    "                        y = linear(query, attention_vec_size, True)\n",
    "                        y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n",
    "                        \n",
    "                        # Attention mask is a softmax of v^T * tanh(...).\n",
    "                        s = math_ops.reduce_sum(v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n",
    "                        a = nn_ops.softmax(s)\n",
    "                        \n",
    "                        # Now calculate the attention-weighted vector d.\n",
    "                        d = math_ops.reduce_sum(array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden, [1, 2])\n",
    "                        ds.append(array_ops.reshape(d, [-1, attn_size]))\n",
    "                        \n",
    "                return ds\n",
    "\n",
    "            outputs = []\n",
    "            prev = None\n",
    "            batch_attn_size = array_ops.pack([batch_size_, attn_size])\n",
    "            attns = [tf.cond(use_initial > 0, lambda: array_ops.zeros(batch_attn_size, dtype=dtypes.float32), lambda: supplied_attns) for _ in xrange(1)]\n",
    "            \n",
    "            for a in attns:  # Ensure the second shape of attention vectors is set.\n",
    "                a.set_shape([None, attn_size])\n",
    "            \n",
    "            with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "                #inp = tf.cond(use_initial > 0, lambda: decoder_inputs[0], lambda: loop_function(supplied_prev, 0))\n",
    "                inp = decoder_inputs[0]\n",
    "                \n",
    "            input_size = inp.get_shape().with_rank(2)[1]\n",
    "            \n",
    "            if input_size.value is None:\n",
    "                raise ValueError(\"Could not infer input size from input: %s\" % inp.name)\n",
    "                \n",
    "            x = linear([inp] + attns, input_size, True)\n",
    "            \n",
    "            # Run the RNN.\n",
    "            cell_output, state = cell(x, state)\n",
    "            \n",
    "            # Run the attention mechanism.\n",
    "            attns = attention(state)\n",
    "            \n",
    "            with variable_scope.variable_scope(\"AttnOutputProjection\"):\n",
    "                output = linear([cell_output] + attns, output_size, True)\n",
    "                \n",
    "        ntf_dec_outputs, ntf_dec_state, ntf_dec_attns = output, state, attns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.all_variables(), max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore variables\n",
    "Optionally restore variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resume_at = 650\n",
    "\n",
    "if resume_at > 0:\n",
    "    saver.restore(sess, 'checkpoints/saved-model-1off-attn-' + str(resume_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Do not initialize variables if restoring from a saved file.  \n",
    "**Warning:** epoch numbers start from 0, and *will* overwrite your old saves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if resume_at == 0:\n",
    "    sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080\n",
      "640\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_x = np.load('data/mutated-train.npy')\n",
    "train_y = np.load('data/fixes-train.npy')\n",
    "\n",
    "assert(len(train_x) == len(train_y))\n",
    "num_train = len(train_x)\n",
    "print num_train\n",
    "\n",
    "valid_x = np.load('data/mutated-validation.npy')\n",
    "valid_y = np.load('data/fixes-validation.npy')\n",
    "\n",
    "assert(len(valid_x) == len(valid_y))\n",
    "num_validation = len(valid_x)\n",
    "print num_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validate_batch(batch_id):\n",
    "    X = valid_x[batch_id*batch_size:(batch_id+1)*batch_size]\n",
    "    Y = valid_y[batch_id*batch_size:(batch_id+1)*batch_size]\n",
    "    \n",
    "    # Dimshuffle to seq_len * batch_size\n",
    "    X = np.array(X).T\n",
    "    Y = np.array(Y).T\n",
    "    \n",
    "    Y_hat = np.zeros(out_seq_length)\n",
    "    \n",
    "    q = Queue.PriorityQueue()\n",
    "    q.put((0, [], []), False)\n",
    "            \n",
    "    feed_dict = {enc_inp[t]: X[t] for t in range(seq_length)}\n",
    "    \n",
    "    # Do it one symbol at a time\n",
    "    for t in range(out_seq_length):\n",
    "        q_next = Queue.PriorityQueue()\n",
    "        \n",
    "        # Discard entries\n",
    "        while q.qsize() > 30:\n",
    "            q.get()\n",
    "        \n",
    "        while not q.empty():\n",
    "            l, s, _ = q.get()\n",
    "            \n",
    "            for t2 in range(len(s)+1):\n",
    "                if t2 == 0:\n",
    "                    feed_dict.update({use_initial: 1})\n",
    "                    feed_dict.update({dec_inp[0]: [0]})\n",
    "                    # Useless:\n",
    "                    feed_dict.update({supplied_state: np.zeros((1, memory_dim * 2))})\n",
    "                    feed_dict.update({supplied_prev: np.zeros((1, vocab_size + 1))})\n",
    "                    feed_dict.update({supplied_attns: np.zeros((1, memory_dim))})\n",
    "                else:\n",
    "                    feed_dict.update({use_initial: 0})\n",
    "                    feed_dict.update({supplied_state: prev_state})\n",
    "                    feed_dict.update({supplied_prev: prev_output})\n",
    "                    feed_dict.update({supplied_attns: prev_attns})\n",
    "                    feed_dict.update({dec_inp[0]: [s[t2-1]]})\n",
    "\n",
    "                prev_output, prev_state, prev_attns = sess.run((output, state, ntf_dec_attns), feed_dict)\n",
    "                Y_hat[t2] = [logits_t.argmax() for logits_t in prev_output][0]\n",
    "            \n",
    "            probs = (prev_output[0]-np.min(prev_output[0]))\n",
    "            probs = probs/np.sum(probs)\n",
    "            bests = np.argpartition(probs, -5)[-5:]\n",
    "            \n",
    "            for best in bests:\n",
    "                q_next.put((l + np.log(probs[best]), s + [best], Y_hat))\n",
    "                \n",
    "        q = q_next\n",
    "    \n",
    "    # Yes, there are 5 of them!\n",
    "    Y_hats = []\n",
    "    \n",
    "    # Discard entries\n",
    "    while q.qsize() > 30:\n",
    "        q.get()\n",
    "        \n",
    "    while not q.empty():\n",
    "        _, _, this_y_hat = q.get()\n",
    "        Y_hats.append(this_y_hat)\n",
    "    \n",
    "    return X, Y.T[0], Y_hats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Sequence Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "0/2\n",
      "0/3\n"
     ]
    }
   ],
   "source": [
    "accurate = 0\n",
    "total = 0\n",
    "\n",
    "for i in [64, 412, 571]:\n",
    "    X, Y, Y_hats = validate_batch(i)\n",
    "    \n",
    "    for Y_hat in Y_hats:\n",
    "        if np.array_equal(Y, Y_hat):\n",
    "            accurate += 1\n",
    "            break\n",
    "    \n",
    "    total += batch_size\n",
    "    print \"%d/%d\" % (accurate, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(accurate)/total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
