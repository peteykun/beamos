{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf, beamos\n",
    "import Queue, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't use all the VRAM!\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.15)\n",
    "#sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# We're on CPU!\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data variables\n",
    "seq_length = 16\n",
    "out_seq_length = 16\n",
    "batch_size = 1\n",
    "vocab_size = 26 + 1               # 0 for padding\n",
    "embedding_dim = 26\n",
    "\n",
    "# Network hyperparameters\n",
    "memory_dim = 200\n",
    "num_layers = 1\n",
    "\n",
    "# Training variables\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First build input placeholders and constants. The `seq2seq` API generally deals with lists of tensors, where each tensor represents a single timestep. An input to an embedding encoder, for example, would be a list of `seq_length` tensors, each of which is of dimension `batch_size` (specifying the embedding indices to input at a particular timestep).\n",
    "\n",
    "We allocate a `labels` placeholder using the same convention. A `weights` constant specifies cross-entropy weights for each label at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_inp = [tf.placeholder(tf.int32, shape=(None,), name=\"inp%i\" % t) for t in range(seq_length)]\n",
    "labels = [tf.placeholder(tf.int32, shape=(None,), name=\"labels%i\" % t) for t in range(out_seq_length)]\n",
    "weights = [tf.ones_like(labels_t, dtype=tf.float32) for labels_t in labels]\n",
    "use_initial = tf.placeholder(tf.int32, shape=[], name=\"use_initial\")\n",
    "supplied_prev  = tf.placeholder(tf.float32, shape=(1, 28), name=\"supplied_prev\")\n",
    "supplied_state = tf.placeholder(tf.float32, shape=(1, memory_dim*2), name=\"supplied_state\")\n",
    "supplied_attns = tf.placeholder(tf.float32, shape=(1, memory_dim), name=\"supplied_attns\")\n",
    "supplied_encoder_state = tf.placeholder(tf.float32, shape=(1, memory_dim*2), name=\"supplied_encoder_state\")\n",
    "supplied_encoder_outputs = [tf.placeholder(tf.float32, shape=(1, memory_dim), name=\"supplied_encoder_outputs%i\" % t) for t in range(seq_length)]\n",
    "\n",
    "# Decoder input: DO NOT PREPEND GO TOKEN! (Feed it in during sampling.)\n",
    "dec_inp = [tf.placeholder(tf.int32, shape=(None,), name=\"dec_inp%i\" % t) for t in range(out_seq_length)]\n",
    "\n",
    "# Initial memory value for recurrence.\n",
    "prev_mem = tf.zeros((batch_size, memory_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the sequence-to-sequence graph.\n",
    "\n",
    "There is a **lot** of complexity hidden in these two calls, and it's certainly worth digging into both in order to really understand how this is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7faefcd7b390>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "constituent_cell = tf.nn.rnn_cell.BasicLSTMCell(memory_dim)\n",
    "\n",
    "if num_layers > 1:\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([constituent_cell] * num_layers)\n",
    "else:\n",
    "    cell = constituent_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_state = beamos.embedding_attention_encoder_seq2seq(enc_inp, cell, vocab_size+1, embedding_dim)\n",
    "output, state, attns = beamos.embedding_attention_seq2seq_beam(dec_inp, use_initial, supplied_prev, supplied_state, supplied_attns, cell, vocab_size+1, embedding_dim, supplied_encoder_outputs, supplied_encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.all_variables(), max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore variables\n",
    "Optionally restore variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resume_at = 650\n",
    "\n",
    "if resume_at > 0:\n",
    "    saver.restore(sess, 'checkpoints/saved-model-1off-attn-' + str(resume_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Do not initialize variables if restoring from a saved file.  \n",
    "**Warning:** epoch numbers start from 0, and *will* overwrite your old saves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if resume_at == 0:\n",
    "    sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080\n",
      "640\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_x = np.load('data/mutated-train.npy')\n",
    "train_y = np.load('data/fixes-train.npy')\n",
    "\n",
    "assert(len(train_x) == len(train_y))\n",
    "num_train = len(train_x)\n",
    "print num_train\n",
    "\n",
    "valid_x = np.load('data/mutated-validation.npy')\n",
    "valid_y = np.load('data/fixes-validation.npy')\n",
    "\n",
    "assert(len(valid_x) == len(valid_y))\n",
    "num_validation = len(valid_x)\n",
    "print num_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beam_size = 5\n",
    "\n",
    "def validate_batch(batch_id):\n",
    "    X = valid_x[batch_id*batch_size:(batch_id+1)*batch_size]\n",
    "    Y = valid_y[batch_id*batch_size:(batch_id+1)*batch_size]\n",
    "    \n",
    "    # Dimshuffle to seq_len * batch_size\n",
    "    X = np.array(X).T\n",
    "    Y = np.array(Y).T\n",
    "    \n",
    "    Y_hat = np.zeros(out_seq_length)\n",
    "    \n",
    "    q = Queue.PriorityQueue()\n",
    "    q.put((0, [], []), False)\n",
    "            \n",
    "    feed_dict = {enc_inp[t]: X[t] for t in range(seq_length)}\n",
    "    \n",
    "    enc_outs = [0] * seq_length\n",
    "\n",
    "    lhs = \"\"\n",
    "    rhs = \"\"\n",
    "\n",
    "    for i in range(seq_length):\n",
    "        lhs += \"enc_outs[%d], \" % i\n",
    "        rhs += \"encoder_outputs[%d], \" % i\n",
    "\n",
    "    statement = \"%s, enc_state = sess.run((%s, encoder_state), feed_dict)\" % (lhs[:-2], rhs[:-2])\n",
    "    exec(statement) in globals(), locals()\n",
    "    \n",
    "    feed_dict = {supplied_encoder_outputs[t]: enc_outs[t] for t in range(seq_length)}\n",
    "    feed_dict.update({supplied_encoder_state: enc_state})\n",
    "    \n",
    "    # Do it one symbol at a time\n",
    "    for t in range(out_seq_length):\n",
    "        q_next = Queue.PriorityQueue()\n",
    "        \n",
    "        # Discard entries\n",
    "        while q.qsize() > beam_size:\n",
    "            q.get()\n",
    "        \n",
    "        while not q.empty():\n",
    "            l, s, _ = q.get()\n",
    "            \n",
    "            for t2 in range(len(s)+1):\n",
    "                if t2 == 0:\n",
    "                    feed_dict.update({use_initial: 1})\n",
    "                    feed_dict.update({dec_inp[0]: [0]})\n",
    "                    # Useless:\n",
    "                    feed_dict.update({supplied_state: np.zeros((1, memory_dim * 2))})\n",
    "                    feed_dict.update({supplied_prev: np.zeros((1, vocab_size + 1))})\n",
    "                    feed_dict.update({supplied_attns: np.zeros((1, memory_dim))})\n",
    "                else:\n",
    "                    feed_dict.update({use_initial: 0})\n",
    "                    feed_dict.update({supplied_state: prev_state})\n",
    "                    feed_dict.update({supplied_prev: prev_output})\n",
    "                    feed_dict.update({supplied_attns: prev_attns})\n",
    "                    feed_dict.update({dec_inp[0]: [s[t2-1]]})\n",
    "\n",
    "                prev_output, prev_state, prev_attns = sess.run((output, state, attns), feed_dict)\n",
    "                Y_hat[t2] = [logits_t.argmax() for logits_t in prev_output][0]\n",
    "            \n",
    "            probs = (prev_output[0]-np.min(prev_output[0]))\n",
    "            probs = probs/np.sum(probs)\n",
    "            bests = np.argpartition(probs, -beam_size)[-beam_size:]\n",
    "            \n",
    "            for best in bests:\n",
    "                q_next.put((l + np.log(probs[best]), s + [best], Y_hat))\n",
    "                \n",
    "        q = q_next\n",
    "    \n",
    "    # Yes, there are `beam_size` of them!\n",
    "    Y_hats = []\n",
    "    \n",
    "    # Discard entries\n",
    "    while q.qsize() > beam_size:\n",
    "        q.get()\n",
    "        \n",
    "    while not q.empty():\n",
    "        _, this_y_hat, _ = q.get()\n",
    "        Y_hats.append(this_y_hat)\n",
    " \n",
    "    return X, Y.T[0], Y_hats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Sequence Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "2/2\n",
      "3/3\n"
     ]
    }
   ],
   "source": [
    "accurate = 0\n",
    "total = 0\n",
    "\n",
    "for i in [64, 412, 571]:\n",
    "    X, Y, Y_hats = validate_batch(i)\n",
    "    \n",
    "    for Y_hat in Y_hats:\n",
    "        if np.array_equal(Y, Y_hat):\n",
    "            accurate += 1\n",
    "            break\n",
    "    \n",
    "    total += batch_size\n",
    "    print \"%d/%d\" % (accurate, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(accurate)/total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
