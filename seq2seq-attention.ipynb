{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't use all the VRAM!\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.15)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data variables\n",
    "seq_length = 16\n",
    "out_seq_length = 16\n",
    "batch_size = 32\n",
    "vocab_size = 26 + 1               # 0 for padding\n",
    "embedding_dim = 26\n",
    "\n",
    "# Network hyperparameters\n",
    "memory_dim = 200\n",
    "num_layers = 1\n",
    "\n",
    "# Training variables\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First build input placeholders and constants. The `seq2seq` API generally deals with lists of tensors, where each tensor represents a single timestep. An input to an embedding encoder, for example, would be a list of `seq_length` tensors, each of which is of dimension `batch_size` (specifying the embedding indices to input at a particular timestep).\n",
    "\n",
    "We allocate a `labels` placeholder using the same convention. A `weights` constant specifies cross-entropy weights for each label at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_inp = [tf.placeholder(tf.int32, shape=(None,), name=\"inp%i\" % t) for t in range(seq_length)]\n",
    "labels = [tf.placeholder(tf.int32, shape=(None,), name=\"labels%i\" % t) for t in range(out_seq_length)]\n",
    "weights = [tf.ones_like(labels_t, dtype=tf.float32) for labels_t in labels]\n",
    "\n",
    "# Decoder input: prepend some \"GO\" token and drop the final\n",
    "# token of the decoder output\n",
    "dec_inp = ([tf.zeros_like(enc_inp[0], dtype=np.int32, name=\"GO\")] +\n",
    "           [tf.placeholder(tf.int32, shape=(None,), name=\"dec_inp%i\" % t) for t in range(out_seq_length - 1)])\n",
    "\n",
    "# Initial memory value for recurrence.\n",
    "prev_mem = tf.zeros((batch_size, memory_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the sequence-to-sequence graph.\n",
    "\n",
    "There is a **lot** of complexity hidden in these two calls, and it's certainly worth digging into both in order to really understand how this is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fbff3a65190>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "constituent_cell = tf.nn.rnn_cell.BasicLSTMCell(memory_dim)\n",
    "\n",
    "if num_layers > 1:\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([constituent_cell] * num_layers)\n",
    "else:\n",
    "    cell = constituent_cell\n",
    "\n",
    "# Without teacher forcing, with attention\n",
    "ntf_dec_outputs, ntf_dec_memory = tf.nn.seq2seq.embedding_attention_seq2seq(enc_inp, dec_inp, cell, vocab_size+1, vocab_size+1, embedding_dim, feed_previous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a standard sequence loss function: mean cross-entropy over each item of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntf_loss = tf.nn.seq2seq.sequence_loss(ntf_dec_outputs, labels, weights, vocab_size + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "momentum = 0.9\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(ntf_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.all_variables(), max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore variables\n",
    "Optionally restore variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resume_at = 0\n",
    "\n",
    "if resume_at > 0:\n",
    "    saver.restore(sess, 'checkpoints/saved-model-1off-attn-' + str(resume_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Do not initialize variables if restoring from a saved file.  \n",
    "**Warning:** epoch numbers start from 0, and *will* overwrite your old saves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if resume_at == 0:\n",
    "    sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080\n",
      "640\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_x = np.load('data/mutated-train.npy')\n",
    "train_y = np.load('data/fixes-train.npy')\n",
    "\n",
    "assert(len(train_x) == len(train_y))\n",
    "num_train = len(train_x)\n",
    "print num_train\n",
    "\n",
    "valid_x = np.load('data/mutated-validation.npy')\n",
    "valid_y = np.load('data/fixes-validation.npy')\n",
    "\n",
    "assert(len(valid_x) == len(valid_y))\n",
    "num_validation = len(valid_x)\n",
    "print num_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_batch(batch_id):\n",
    "    X = valid_x[batch_id*batch_size:(batch_id+1)*batch_size]\n",
    "    Y = valid_y[batch_id*batch_size:(batch_id+1)*batch_size]\n",
    "    \n",
    "    # Dimshuffle to seq_len * batch_size\n",
    "    X = np.array(X).T\n",
    "    Y = np.array(Y).T\n",
    "\n",
    "    feed_dict = {enc_inp[t]: X[t] for t in range(seq_length)}\n",
    "    feed_dict.update({labels[t]: Y[t] for t in range(out_seq_length)})\n",
    "    feed_dict.update({dec_inp[t]: Y[t] for t in range(out_seq_length - 1)})\n",
    "\n",
    "    loss_t = sess.run([ntf_loss], feed_dict)\n",
    "    dec_outputs_batch = sess.run(ntf_dec_outputs, feed_dict)\n",
    "    Y_hat = [logits_t.argmax(axis=1) for logits_t in dec_outputs_batch]\n",
    "    \n",
    "    accuracy = float(np.count_nonzero(np.equal(Y, Y_hat)))/np.prod(np.shape(Y))\n",
    "    \n",
    "    return loss_t, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_batch(batch_id):\n",
    "    X = train_x[batch_id*batch_size:(batch_id+1)*batch_size]\n",
    "    Y = train_y[batch_id*batch_size:(batch_id+1)*batch_size]\n",
    "    \n",
    "    # Dimshuffle to seq_len * batch_size\n",
    "    X = np.array(X).T\n",
    "    Y = np.array(Y).T\n",
    "\n",
    "    feed_dict = {enc_inp[t]: X[t] for t in range(seq_length)}\n",
    "    feed_dict.update({labels[t]: Y[t] for t in range(out_seq_length)})\n",
    "    feed_dict.update({dec_inp[t]: Y[t] for t in range(out_seq_length - 1)})\n",
    "\n",
    "    _, loss_t = sess.run([train_op, ntf_loss], feed_dict)\n",
    "    return loss_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1\tEpoch: 0.0\tTraining: 3.39436\n",
      "Step: 2\tEpoch: 0.0153846153846\tTraining: 3.22046\n",
      "Step: 3\tEpoch: 0.0307692307692\tTraining: 3.13181\n",
      "Step: 4\tEpoch: 0.0461538461538\tTraining: 2.88012\n",
      "Step: 5\tEpoch: 0.0615384615385\tTraining: 2.8712\n",
      "Step: 6\tEpoch: 0.0769230769231\tTraining: 2.60116\n",
      "Step: 7\tEpoch: 0.0923076923077\tTraining: 2.60308\n",
      "Step: 8\tEpoch: 0.107692307692\tTraining: 2.60628\n",
      "Step: 9\tEpoch: 0.123076923077\tTraining: 2.74845\n",
      "Step: 10\tEpoch: 0.138461538462\tTraining: 2.82863\n",
      "Step: 11\tEpoch: 0.153846153846\tTraining: 2.75379\n",
      "Step: 12\tEpoch: 0.169230769231\tTraining: 2.53044\n",
      "Step: 13\tEpoch: 0.184615384615\tTraining: 2.73766\n",
      "Step: 14\tEpoch: 0.2\tTraining: 2.5248\n",
      "Step: 15\tEpoch: 0.215384615385\tTraining: 2.67623\n",
      "Step: 16\tEpoch: 0.230769230769\tTraining: 2.55156\n",
      "Step: 17\tEpoch: 0.246153846154\tTraining: 2.54372\n",
      "Step: 18\tEpoch: 0.261538461538\tTraining: 2.45925\n",
      "Step: 19\tEpoch: 0.276923076923\tTraining: 2.32565\n",
      "Step: 20\tEpoch: 0.292307692308\tTraining: 2.47849\n",
      "Step: 21\tEpoch: 0.307692307692\tTraining: 2.50453\n",
      "Step: 22\tEpoch: 0.323076923077\tTraining: 2.49736\n",
      "Step: 23\tEpoch: 0.338461538462\tTraining: 2.46207\n",
      "Step: 24\tEpoch: 0.353846153846\tTraining: 2.33253\n",
      "Step: 25\tEpoch: 0.369230769231\tTraining: 2.40631\n",
      "Step: 26\tEpoch: 0.384615384615\tTraining: 2.43379\n",
      "Step: 27\tEpoch: 0.4\tTraining: 2.46793\n",
      "Step: 28\tEpoch: 0.415384615385\tTraining: 2.53285\n",
      "Step: 29\tEpoch: 0.430769230769\tTraining: 2.38705\n",
      "Step: 30\tEpoch: 0.446153846154\tTraining: 2.16062\n",
      "Step: 31\tEpoch: 0.461538461538\tTraining: 2.53634\n",
      "Step: 32\tEpoch: 0.476923076923\tTraining: 2.22895\n",
      "Step: 33\tEpoch: 0.492307692308\tTraining: 2.44684\n",
      "Step: 34\tEpoch: 0.507692307692\tTraining: 2.44668\n",
      "Step: 35\tEpoch: 0.523076923077\tTraining: 2.06429\n",
      "Step: 36\tEpoch: 0.538461538462\tTraining: 2.18352\n",
      "Step: 37\tEpoch: 0.553846153846\tTraining: 2.29563\n",
      "Step: 38\tEpoch: 0.569230769231\tTraining: 2.07279\n",
      "Step: 39\tEpoch: 0.584615384615\tTraining: 2.48015\n",
      "Step: 40\tEpoch: 0.6\tTraining: 2.47856\n",
      "Step: 41\tEpoch: 0.615384615385\tTraining: 2.21669\n",
      "Step: 42\tEpoch: 0.630769230769\tTraining: 2.40204\n",
      "Step: 43\tEpoch: 0.646153846154\tTraining: 2.3834\n",
      "Step: 44\tEpoch: 0.661538461538\tTraining: 2.2985\n",
      "Step: 45\tEpoch: 0.676923076923\tTraining: 2.0626\n",
      "Step: 46\tEpoch: 0.692307692308\tTraining: 2.3738\n",
      "Step: 47\tEpoch: 0.707692307692\tTraining: 2.21223\n",
      "Step: 48\tEpoch: 0.723076923077\tTraining: 2.23599\n",
      "Step: 49\tEpoch: 0.738461538462\tTraining: 2.26266\n",
      "Step: 50\tEpoch: 0.753846153846\tTraining: 2.21536\n",
      "Step: 51\tEpoch: 0.769230769231\tTraining: 2.23123\n",
      "Step: 52\tEpoch: 0.784615384615\tTraining: 2.00523\n",
      "Step: 53\tEpoch: 0.8\tTraining: 2.43446\n",
      "Step: 54\tEpoch: 0.815384615385\tTraining: 2.16861\n",
      "Step: 55\tEpoch: 0.830769230769\tTraining: 2.06118\n",
      "Step: 56\tEpoch: 0.846153846154\tTraining: 2.02202\n",
      "Step: 57\tEpoch: 0.861538461538\tTraining: 2.26078\n",
      "Step: 58\tEpoch: 0.876923076923\tTraining: 2.24002\n",
      "Step: 59\tEpoch: 0.892307692308\tTraining: 2.14555\n",
      "Step: 60\tEpoch: 0.907692307692\tTraining: 2.13468\n",
      "Step: 61\tEpoch: 0.923076923077\tTraining: 2.25088\n",
      "Step: 62\tEpoch: 0.938461538462\tTraining: 2.12016\n",
      "Step: 63\tEpoch: 0.953846153846\tTraining: 1.93024\n",
      "Step: 64\tEpoch: 0.969230769231\tTraining: 2.17637\n",
      "Step: 65\tEpoch: 0.984615384615\tTraining: 2.02859\n",
      "Step: 65\tEpoch: 1\tTraining: 2.41168\tValidation loss: 2.08024\tValidation acc: 0.45322265625\n",
      "Step: 66\tEpoch: 1.0\tTraining: 2.03857\n",
      "Step: 67\tEpoch: 1.01538461538\tTraining: 2.02249\n",
      "Step: 68\tEpoch: 1.03076923077\tTraining: 2.2258\n",
      "Step: 69\tEpoch: 1.04615384615\tTraining: 2.01241\n",
      "Step: 70\tEpoch: 1.06153846154\tTraining: 2.20264\n",
      "Step: 71\tEpoch: 1.07692307692\tTraining: 1.90558\n",
      "Step: 72\tEpoch: 1.09230769231\tTraining: 1.89475\n",
      "Step: 73\tEpoch: 1.10769230769\tTraining: 1.83849\n",
      "Step: 74\tEpoch: 1.12307692308\tTraining: 2.0362\n",
      "Step: 75\tEpoch: 1.13846153846\tTraining: 2.20921\n",
      "Step: 76\tEpoch: 1.15384615385\tTraining: 2.13597\n",
      "Step: 77\tEpoch: 1.16923076923\tTraining: 1.82923\n",
      "Step: 78\tEpoch: 1.18461538462\tTraining: 2.13312\n",
      "Step: 79\tEpoch: 1.2\tTraining: 1.88704\n",
      "Step: 80\tEpoch: 1.21538461538\tTraining: 2.09858\n",
      "Step: 81\tEpoch: 1.23076923077\tTraining: 1.92906\n",
      "Step: 82\tEpoch: 1.24615384615\tTraining: 1.94151\n",
      "Step: 83\tEpoch: 1.26153846154\tTraining: 1.89204\n",
      "Step: 84\tEpoch: 1.27692307692\tTraining: 1.76562\n",
      "Step: 85\tEpoch: 1.29230769231\tTraining: 1.89628\n",
      "Step: 86\tEpoch: 1.30769230769\tTraining: 1.93205\n",
      "Step: 87\tEpoch: 1.32307692308\tTraining: 1.90575\n",
      "Step: 88\tEpoch: 1.33846153846\tTraining: 1.87296\n",
      "Step: 89\tEpoch: 1.35384615385\tTraining: 1.73114\n",
      "Step: 90\tEpoch: 1.36923076923\tTraining: 1.78582\n",
      "Step: 91\tEpoch: 1.38461538462\tTraining: 1.78583\n",
      "Step: 92\tEpoch: 1.4\tTraining: 1.84556\n",
      "Step: 93\tEpoch: 1.41538461538\tTraining: 1.87388\n",
      "Step: 94\tEpoch: 1.43076923077\tTraining: 1.78505\n",
      "Step: 95\tEpoch: 1.44615384615\tTraining: 1.55144\n",
      "Step: 96\tEpoch: 1.46153846154\tTraining: 1.85512\n",
      "Step: 97\tEpoch: 1.47692307692\tTraining: 1.56085\n",
      "Step: 98\tEpoch: 1.49230769231\tTraining: 1.91715\n",
      "Step: 99\tEpoch: 1.50769230769\tTraining: 1.83535\n",
      "Step: 100\tEpoch: 1.52307692308\tTraining: 1.46087\n",
      "Step: 101\tEpoch: 1.53846153846\tTraining: 1.60486\n",
      "Step: 102\tEpoch: 1.55384615385\tTraining: 1.62576\n",
      "Step: 103\tEpoch: 1.56923076923\tTraining: 1.49724\n",
      "Step: 104\tEpoch: 1.58461538462\tTraining: 1.70765\n",
      "Step: 105\tEpoch: 1.6\tTraining: 1.71155\n",
      "Step: 106\tEpoch: 1.61538461538\tTraining: 1.47863\n",
      "Step: 107\tEpoch: 1.63076923077\tTraining: 1.61445\n",
      "Step: 108\tEpoch: 1.64615384615\tTraining: 1.56526\n",
      "Step: 109\tEpoch: 1.66153846154\tTraining: 1.50738\n",
      "Step: 110\tEpoch: 1.67692307692\tTraining: 1.29944\n",
      "Step: 111\tEpoch: 1.69230769231\tTraining: 1.50608\n",
      "Step: 112\tEpoch: 1.70769230769\tTraining: 1.43798\n",
      "Step: 113\tEpoch: 1.72307692308\tTraining: 1.38979\n",
      "Step: 114\tEpoch: 1.73846153846\tTraining: 1.43439\n",
      "Step: 115\tEpoch: 1.75384615385\tTraining: 1.3401\n",
      "Step: 116\tEpoch: 1.76923076923\tTraining: 1.34355\n",
      "Step: 117\tEpoch: 1.78461538462\tTraining: 1.16465\n",
      "Step: 118\tEpoch: 1.8\tTraining: 1.52624\n",
      "Step: 119\tEpoch: 1.81538461538\tTraining: 1.34748\n",
      "Step: 120\tEpoch: 1.83076923077\tTraining: 1.13457\n",
      "Step: 121\tEpoch: 1.84615384615\tTraining: 1.11833\n",
      "Step: 122\tEpoch: 1.86153846154\tTraining: 1.27091\n",
      "Step: 123\tEpoch: 1.87692307692\tTraining: 1.19119\n",
      "Step: 124\tEpoch: 1.89230769231\tTraining: 1.41921\n",
      "Step: 125\tEpoch: 1.90769230769\tTraining: 1.60848\n",
      "Step: 126\tEpoch: 1.92307692308\tTraining: 1.88134\n",
      "Step: 127\tEpoch: 1.93846153846\tTraining: 1.54139\n",
      "Step: 128\tEpoch: 1.95384615385\tTraining: 1.4468\n",
      "Step: 129\tEpoch: 1.96923076923\tTraining: 1.52078\n",
      "Step: 130\tEpoch: 1.98461538462\tTraining: 1.3906\n",
      "Step: 130\tEpoch: 2\tTraining: 1.69568\tValidation loss: 1.3939\tValidation acc: 0.641796875\n",
      "Step: 131\tEpoch: 2.0\tTraining: 1.35291\n",
      "Step: 132\tEpoch: 2.01538461538\tTraining: 1.20571\n",
      "Step: 133\tEpoch: 2.03076923077\tTraining: 1.24297\n",
      "Step: 134\tEpoch: 2.04615384615\tTraining: 1.40922\n",
      "Step: 135\tEpoch: 2.06153846154\tTraining: 1.34267\n",
      "Step: 136\tEpoch: 2.07692307692\tTraining: 1.0606\n",
      "Step: 137\tEpoch: 2.09230769231\tTraining: 1.01609\n",
      "Step: 138\tEpoch: 2.10769230769\tTraining: 0.913408\n",
      "Step: 139\tEpoch: 2.12307692308\tTraining: 1.11482\n",
      "Step: 140\tEpoch: 2.13846153846\tTraining: 1.18973\n",
      "Step: 141\tEpoch: 2.15384615385\tTraining: 1.20582\n",
      "Step: 142\tEpoch: 2.16923076923\tTraining: 0.970372\n",
      "Step: 143\tEpoch: 2.18461538462\tTraining: 1.12719\n",
      "Step: 144\tEpoch: 2.2\tTraining: 0.931368\n",
      "Step: 145\tEpoch: 2.21538461538\tTraining: 1.10714\n",
      "Step: 146\tEpoch: 2.23076923077\tTraining: 1.03645\n",
      "Step: 147\tEpoch: 2.24615384615\tTraining: 0.93301\n",
      "Step: 148\tEpoch: 2.26153846154\tTraining: 0.901605\n",
      "Step: 149\tEpoch: 2.27692307692\tTraining: 0.752173\n",
      "Step: 150\tEpoch: 2.29230769231\tTraining: 0.922118\n",
      "Step: 151\tEpoch: 2.30769230769\tTraining: 0.859626\n",
      "Step: 152\tEpoch: 2.32307692308\tTraining: 0.881164\n",
      "Step: 153\tEpoch: 2.33846153846\tTraining: 0.72779\n",
      "Step: 154\tEpoch: 2.35384615385\tTraining: 0.716096\n",
      "Step: 155\tEpoch: 2.36923076923\tTraining: 0.672007\n",
      "Step: 156\tEpoch: 2.38461538462\tTraining: 0.797076\n",
      "Step: 157\tEpoch: 2.4\tTraining: 0.738903\n",
      "Step: 158\tEpoch: 2.41538461538\tTraining: 0.793677\n",
      "Step: 159\tEpoch: 2.43076923077\tTraining: 0.625216\n",
      "Step: 160\tEpoch: 2.44615384615\tTraining: 0.565887\n",
      "Step: 161\tEpoch: 2.46153846154\tTraining: 0.718606\n",
      "Step: 162\tEpoch: 2.47692307692\tTraining: 0.591756\n",
      "Step: 163\tEpoch: 2.49230769231\tTraining: 0.615799\n",
      "Step: 164\tEpoch: 2.50769230769\tTraining: 0.778536\n",
      "Step: 165\tEpoch: 2.52307692308\tTraining: 0.459655\n",
      "Step: 166\tEpoch: 2.53846153846\tTraining: 0.759759\n",
      "Step: 167\tEpoch: 2.55384615385\tTraining: 0.697338\n",
      "Step: 168\tEpoch: 2.56923076923\tTraining: 0.450953\n",
      "Step: 169\tEpoch: 2.58461538462\tTraining: 0.853213\n",
      "Step: 170\tEpoch: 2.6\tTraining: 0.889805\n",
      "Step: 171\tEpoch: 2.61538461538\tTraining: 0.732077\n",
      "Step: 172\tEpoch: 2.63076923077\tTraining: 1.51949\n",
      "Step: 173\tEpoch: 2.64615384615\tTraining: 1.05923\n",
      "Step: 174\tEpoch: 2.66153846154\tTraining: 0.891238\n",
      "Step: 175\tEpoch: 2.67692307692\tTraining: 1.01735\n",
      "Step: 176\tEpoch: 2.69230769231\tTraining: 1.05367\n",
      "Step: 177\tEpoch: 2.70769230769\tTraining: 0.775658\n",
      "Step: 178\tEpoch: 2.72307692308\tTraining: 0.820459\n",
      "Step: 179\tEpoch: 2.73846153846\tTraining: 0.859388\n",
      "Step: 180\tEpoch: 2.75384615385\tTraining: 0.946264\n",
      "Step: 181\tEpoch: 2.76923076923\tTraining: 1.00638\n",
      "Step: 182\tEpoch: 2.78461538462\tTraining: 0.56906\n",
      "Step: 183\tEpoch: 2.8\tTraining: 0.972893\n",
      "Step: 184\tEpoch: 2.81538461538\tTraining: 0.657154\n",
      "Step: 185\tEpoch: 2.83076923077\tTraining: 0.475934\n",
      "Step: 186\tEpoch: 2.84615384615\tTraining: 0.596359\n",
      "Step: 187\tEpoch: 2.86153846154\tTraining: 0.663453\n",
      "Step: 188\tEpoch: 2.87692307692\tTraining: 0.687395\n",
      "Step: 189\tEpoch: 2.89230769231\tTraining: 0.60772\n",
      "Step: 190\tEpoch: 2.90769230769\tTraining: 0.486575\n",
      "Step: 191\tEpoch: 2.92307692308\tTraining: 0.537922\n",
      "Step: 192\tEpoch: 2.93846153846\tTraining: 0.46274\n",
      "Step: 193\tEpoch: 2.95384615385\tTraining: 0.419035\n",
      "Step: 194\tEpoch: 2.96923076923\tTraining: 0.514593\n",
      "Step: 195\tEpoch: 2.98461538462\tTraining: 0.386607\n",
      "Step: 195\tEpoch: 3\tTraining: 0.840721\tValidation loss: 0.387148\tValidation acc: 0.95419921875\n",
      "Step: 196\tEpoch: 3.0\tTraining: 0.35221\n",
      "Step: 197\tEpoch: 3.01538461538\tTraining: 0.366538\n",
      "Step: 198\tEpoch: 3.03076923077\tTraining: 0.419829\n",
      "Step: 199\tEpoch: 3.04615384615\tTraining: 0.384817\n",
      "Step: 200\tEpoch: 3.06153846154\tTraining: 0.50428\n",
      "Step: 201\tEpoch: 3.07692307692\tTraining: 0.310132\n",
      "Step: 202\tEpoch: 3.09230769231\tTraining: 0.270348\n",
      "Step: 203\tEpoch: 3.10769230769\tTraining: 0.248327\n",
      "Step: 204\tEpoch: 3.12307692308\tTraining: 0.314852\n",
      "Step: 205\tEpoch: 3.13846153846\tTraining: 0.433289\n",
      "Step: 206\tEpoch: 3.15384615385\tTraining: 0.389175\n",
      "Step: 207\tEpoch: 3.16923076923\tTraining: 0.226192\n",
      "Step: 208\tEpoch: 3.18461538462\tTraining: 0.416863\n",
      "Step: 209\tEpoch: 3.2\tTraining: 0.237148\n",
      "Step: 210\tEpoch: 3.21538461538\tTraining: 0.31798\n",
      "Step: 211\tEpoch: 3.23076923077\tTraining: 0.255687\n",
      "Step: 212\tEpoch: 3.24615384615\tTraining: 0.231574\n",
      "Step: 213\tEpoch: 3.26153846154\tTraining: 0.245513\n",
      "Step: 214\tEpoch: 3.27692307692\tTraining: 0.21291\n",
      "Step: 215\tEpoch: 3.29230769231\tTraining: 0.26362\n",
      "Step: 216\tEpoch: 3.30769230769\tTraining: 0.277065\n",
      "Step: 217\tEpoch: 3.32307692308\tTraining: 0.229573\n",
      "Step: 218\tEpoch: 3.33846153846\tTraining: 0.205289\n",
      "Step: 219\tEpoch: 3.35384615385\tTraining: 0.18268\n",
      "Step: 220\tEpoch: 3.36923076923\tTraining: 0.187566\n",
      "Step: 221\tEpoch: 3.38461538462\tTraining: 0.230836\n",
      "Step: 222\tEpoch: 3.4\tTraining: 0.236309\n",
      "Step: 223\tEpoch: 3.41538461538\tTraining: 0.219882\n",
      "Step: 224\tEpoch: 3.43076923077\tTraining: 0.173344\n",
      "Step: 225\tEpoch: 3.44615384615\tTraining: 0.134296\n",
      "Step: 226\tEpoch: 3.46153846154\tTraining: 0.218149\n",
      "Step: 227\tEpoch: 3.47692307692\tTraining: 0.135705\n",
      "Step: 228\tEpoch: 3.49230769231\tTraining: 0.1849\n",
      "Step: 229\tEpoch: 3.50769230769\tTraining: 0.183746\n",
      "Step: 230\tEpoch: 3.52307692308\tTraining: 0.136212\n",
      "Step: 231\tEpoch: 3.53846153846\tTraining: 0.128931\n",
      "Step: 232\tEpoch: 3.55384615385\tTraining: 0.144906\n",
      "Step: 233\tEpoch: 3.56923076923\tTraining: 0.114232\n",
      "Step: 234\tEpoch: 3.58461538462\tTraining: 0.158206\n",
      "Step: 235\tEpoch: 3.6\tTraining: 0.209282\n",
      "Step: 236\tEpoch: 3.61538461538\tTraining: 0.137731\n",
      "Step: 237\tEpoch: 3.63076923077\tTraining: 0.172056\n",
      "Step: 238\tEpoch: 3.64615384615\tTraining: 0.150182\n",
      "Step: 239\tEpoch: 3.66153846154\tTraining: 0.144772\n",
      "Step: 240\tEpoch: 3.67692307692\tTraining: 0.102517\n",
      "Step: 241\tEpoch: 3.69230769231\tTraining: 0.12442\n",
      "Step: 242\tEpoch: 3.70769230769\tTraining: 0.162822\n",
      "Step: 243\tEpoch: 3.72307692308\tTraining: 0.112239\n",
      "Step: 244\tEpoch: 3.73846153846\tTraining: 0.139894\n",
      "Step: 245\tEpoch: 3.75384615385\tTraining: 0.13172\n",
      "Step: 246\tEpoch: 3.76923076923\tTraining: 0.110449\n",
      "Step: 247\tEpoch: 3.78461538462\tTraining: 0.0803233\n",
      "Step: 248\tEpoch: 3.8\tTraining: 0.145391\n",
      "Step: 249\tEpoch: 3.81538461538\tTraining: 0.132434\n",
      "Step: 250\tEpoch: 3.83076923077\tTraining: 0.0984068\n",
      "Step: 251\tEpoch: 3.84615384615\tTraining: 0.0866399\n",
      "Step: 252\tEpoch: 3.86153846154\tTraining: 0.136198\n",
      "Step: 253\tEpoch: 3.87692307692\tTraining: 0.112263\n",
      "Step: 254\tEpoch: 3.89230769231\tTraining: 0.0928082\n",
      "Step: 255\tEpoch: 3.90769230769\tTraining: 0.0918228\n",
      "Step: 256\tEpoch: 3.92307692308\tTraining: 0.108147\n",
      "Step: 257\tEpoch: 3.93846153846\tTraining: 0.0754365\n",
      "Step: 258\tEpoch: 3.95384615385\tTraining: 0.0693597\n",
      "Step: 259\tEpoch: 3.96923076923\tTraining: 0.0788443\n",
      "Step: 260\tEpoch: 3.98461538462\tTraining: 0.0862264\n",
      "Step: 260\tEpoch: 4\tTraining: 0.199623\tValidation loss: 0.0874839\tValidation acc: 0.99375\n",
      "Step: 261\tEpoch: 4.0\tTraining: 0.0656233\n",
      "Step: 262\tEpoch: 4.01538461538\tTraining: 0.0812651\n",
      "Step: 263\tEpoch: 4.03076923077\tTraining: 0.0822747\n",
      "Step: 264\tEpoch: 4.04615384615\tTraining: 0.0808128\n",
      "Step: 265\tEpoch: 4.06153846154\tTraining: 0.122658\n",
      "Step: 266\tEpoch: 4.07692307692\tTraining: 0.0813676\n",
      "Step: 267\tEpoch: 4.09230769231\tTraining: 0.0546474\n",
      "Step: 268\tEpoch: 4.10769230769\tTraining: 0.057477\n",
      "Step: 269\tEpoch: 4.12307692308\tTraining: 0.0741365\n",
      "Step: 270\tEpoch: 4.13846153846\tTraining: 0.115962\n",
      "Step: 271\tEpoch: 4.15384615385\tTraining: 0.107059\n",
      "Step: 272\tEpoch: 4.16923076923\tTraining: 0.0679055\n",
      "Step: 273\tEpoch: 4.18461538462\tTraining: 0.115544\n",
      "Step: 274\tEpoch: 4.2\tTraining: 0.0662482\n",
      "Step: 275\tEpoch: 4.21538461538\tTraining: 0.0840641\n",
      "Step: 276\tEpoch: 4.23076923077\tTraining: 0.083196\n",
      "Step: 277\tEpoch: 4.24615384615\tTraining: 0.0703334\n",
      "Step: 278\tEpoch: 4.26153846154\tTraining: 0.0759279\n",
      "Step: 279\tEpoch: 4.27692307692\tTraining: 0.0609325\n",
      "Step: 280\tEpoch: 4.29230769231\tTraining: 0.0942817\n",
      "Step: 281\tEpoch: 4.30769230769\tTraining: 0.099947\n",
      "Step: 282\tEpoch: 4.32307692308\tTraining: 0.0639248\n",
      "Step: 283\tEpoch: 4.33846153846\tTraining: 0.080732\n",
      "Step: 284\tEpoch: 4.35384615385\tTraining: 0.0552137\n",
      "Step: 285\tEpoch: 4.36923076923\tTraining: 0.0540018\n",
      "Step: 286\tEpoch: 4.38461538462\tTraining: 0.0752452\n",
      "Step: 287\tEpoch: 4.4\tTraining: 0.088131\n",
      "Step: 288\tEpoch: 4.41538461538\tTraining: 0.0634647\n",
      "Step: 289\tEpoch: 4.43076923077\tTraining: 0.056207\n",
      "Step: 290\tEpoch: 4.44615384615\tTraining: 0.048972\n",
      "Step: 291\tEpoch: 4.46153846154\tTraining: 0.0830452\n",
      "Step: 292\tEpoch: 4.47692307692\tTraining: 0.0426567\n",
      "Step: 293\tEpoch: 4.49230769231\tTraining: 0.0612702\n",
      "Step: 294\tEpoch: 4.50769230769\tTraining: 0.0628829\n",
      "Step: 295\tEpoch: 4.52307692308\tTraining: 0.060556\n",
      "Step: 296\tEpoch: 4.53846153846\tTraining: 0.0439693\n",
      "Step: 297\tEpoch: 4.55384615385\tTraining: 0.0480759\n",
      "Step: 298\tEpoch: 4.56923076923\tTraining: 0.0414432\n",
      "Step: 299\tEpoch: 4.58461538462\tTraining: 0.0581502\n",
      "Step: 300\tEpoch: 4.6\tTraining: 0.0945262\n",
      "Step: 301\tEpoch: 4.61538461538\tTraining: 0.0589748\n",
      "Step: 302\tEpoch: 4.63076923077\tTraining: 0.0597194\n",
      "Step: 303\tEpoch: 4.64615384615\tTraining: 0.0788342\n",
      "Step: 304\tEpoch: 4.66153846154\tTraining: 0.0527269\n",
      "Step: 305\tEpoch: 4.67692307692\tTraining: 0.0546762\n",
      "Step: 306\tEpoch: 4.69230769231\tTraining: 0.045994\n",
      "Step: 307\tEpoch: 4.70769230769\tTraining: 0.0731492\n",
      "Step: 308\tEpoch: 4.72307692308\tTraining: 0.0494001\n",
      "Step: 309\tEpoch: 4.73846153846\tTraining: 0.0643441\n",
      "Step: 310\tEpoch: 4.75384615385\tTraining: 0.0564739\n",
      "Step: 311\tEpoch: 4.76923076923\tTraining: 0.0455084\n",
      "Step: 312\tEpoch: 4.78461538462\tTraining: 0.0395871\n",
      "Step: 313\tEpoch: 4.8\tTraining: 0.0793982\n",
      "Step: 314\tEpoch: 4.81538461538\tTraining: 0.0600595\n",
      "Step: 315\tEpoch: 4.83076923077\tTraining: 0.0466684\n",
      "Step: 316\tEpoch: 4.84615384615\tTraining: 0.0502393\n",
      "Step: 317\tEpoch: 4.86153846154\tTraining: 0.0610726\n",
      "Step: 318\tEpoch: 4.87692307692\tTraining: 0.0498975\n",
      "Step: 319\tEpoch: 4.89230769231\tTraining: 0.042861\n",
      "Step: 320\tEpoch: 4.90769230769\tTraining: 0.047425\n",
      "Step: 321\tEpoch: 4.92307692308\tTraining: 0.0580162\n",
      "Step: 322\tEpoch: 4.93846153846\tTraining: 0.0368223\n",
      "Step: 323\tEpoch: 4.95384615385\tTraining: 0.0337527\n",
      "Step: 324\tEpoch: 4.96923076923\tTraining: 0.035507\n",
      "Step: 325\tEpoch: 4.98461538462\tTraining: 0.0463082\n",
      "Step: 325\tEpoch: 5\tTraining: 0.0654084\tValidation loss: 0.0520086\tValidation acc: 0.9955078125\n",
      "Step: 326\tEpoch: 5.0\tTraining: 0.0340824\n",
      "Step: 327\tEpoch: 5.01538461538\tTraining: 0.0442591\n",
      "Step: 328\tEpoch: 5.03076923077\tTraining: 0.0403844\n",
      "Step: 329\tEpoch: 5.04615384615\tTraining: 0.0379141\n",
      "Step: 330\tEpoch: 5.06153846154\tTraining: 0.0680987\n",
      "Step: 331\tEpoch: 5.07692307692\tTraining: 0.0463005\n",
      "Step: 332\tEpoch: 5.09230769231\tTraining: 0.0295672\n",
      "Step: 333\tEpoch: 5.10769230769\tTraining: 0.0282445\n",
      "Step: 334\tEpoch: 5.12307692308\tTraining: 0.0381013\n",
      "Step: 335\tEpoch: 5.13846153846\tTraining: 0.0657076\n",
      "Step: 336\tEpoch: 5.15384615385\tTraining: 0.0552634\n",
      "Step: 337\tEpoch: 5.16923076923\tTraining: 0.03659\n",
      "Step: 338\tEpoch: 5.18461538462\tTraining: 0.0738375\n",
      "Step: 339\tEpoch: 5.2\tTraining: 0.0295141\n",
      "Step: 340\tEpoch: 5.21538461538\tTraining: 0.045852\n",
      "Step: 341\tEpoch: 5.23076923077\tTraining: 0.0582247\n",
      "Step: 342\tEpoch: 5.24615384615\tTraining: 0.0374772\n",
      "Step: 343\tEpoch: 5.26153846154\tTraining: 0.0417079\n",
      "Step: 344\tEpoch: 5.27692307692\tTraining: 0.0314814\n",
      "Step: 345\tEpoch: 5.29230769231\tTraining: 0.0542084\n",
      "Step: 346\tEpoch: 5.30769230769\tTraining: 0.0538304\n",
      "Step: 347\tEpoch: 5.32307692308\tTraining: 0.033331\n",
      "Step: 348\tEpoch: 5.33846153846\tTraining: 0.0341743\n",
      "Step: 349\tEpoch: 5.35384615385\tTraining: 0.0370219\n",
      "Step: 350\tEpoch: 5.36923076923\tTraining: 0.0356207\n",
      "Step: 351\tEpoch: 5.38461538462\tTraining: 0.0357264\n",
      "Step: 352\tEpoch: 5.4\tTraining: 0.0477675\n",
      "Step: 353\tEpoch: 5.41538461538\tTraining: 0.0549629\n",
      "Step: 354\tEpoch: 5.43076923077\tTraining: 0.0283119\n",
      "Step: 355\tEpoch: 5.44615384615\tTraining: 0.0248527\n",
      "Step: 356\tEpoch: 5.46153846154\tTraining: 0.0740218\n",
      "Step: 357\tEpoch: 5.47692307692\tTraining: 0.0218591\n",
      "Step: 358\tEpoch: 5.49230769231\tTraining: 0.0436619\n",
      "Step: 359\tEpoch: 5.50769230769\tTraining: 0.0353538\n",
      "Step: 360\tEpoch: 5.52307692308\tTraining: 0.0352219\n",
      "Step: 361\tEpoch: 5.53846153846\tTraining: 0.0254015\n",
      "Step: 362\tEpoch: 5.55384615385\tTraining: 0.0289132\n",
      "Step: 363\tEpoch: 5.56923076923\tTraining: 0.0553767\n",
      "Step: 364\tEpoch: 5.58461538462\tTraining: 0.0337299\n",
      "Step: 365\tEpoch: 5.6\tTraining: 0.0513982\n",
      "Step: 366\tEpoch: 5.61538461538\tTraining: 0.035579\n",
      "Step: 367\tEpoch: 5.63076923077\tTraining: 0.0482128\n",
      "Step: 368\tEpoch: 5.64615384615\tTraining: 0.0527012\n",
      "Step: 369\tEpoch: 5.66153846154\tTraining: 0.101604\n",
      "Step: 370\tEpoch: 5.67692307692\tTraining: 0.0516992\n",
      "Step: 371\tEpoch: 5.69230769231\tTraining: 0.394731\n",
      "Step: 372\tEpoch: 5.70769230769\tTraining: 3.69303\n",
      "Step: 373\tEpoch: 5.72307692308\tTraining: 4.34872\n",
      "Step: 374\tEpoch: 5.73846153846\tTraining: 4.15366\n",
      "Step: 375\tEpoch: 5.75384615385\tTraining: 3.52388\n",
      "Step: 376\tEpoch: 5.76923076923\tTraining: 3.57751\n",
      "Step: 377\tEpoch: 5.78461538462\tTraining: 2.98766\n",
      "Step: 378\tEpoch: 5.8\tTraining: 3.44054\n",
      "Step: 379\tEpoch: 5.81538461538\tTraining: 2.86092\n",
      "Step: 380\tEpoch: 5.83076923077\tTraining: 2.57075\n",
      "Step: 381\tEpoch: 5.84615384615\tTraining: 2.48992\n",
      "Step: 382\tEpoch: 5.86153846154\tTraining: 2.64684\n",
      "Step: 383\tEpoch: 5.87692307692\tTraining: 2.55701\n",
      "Step: 384\tEpoch: 5.89230769231\tTraining: 2.33997\n",
      "Step: 385\tEpoch: 5.90769230769\tTraining: 2.16708\n",
      "Step: 386\tEpoch: 5.92307692308\tTraining: 2.38904\n",
      "Step: 387\tEpoch: 5.93846153846\tTraining: 2.14728\n",
      "Step: 388\tEpoch: 5.95384615385\tTraining: 2.0462\n",
      "Step: 389\tEpoch: 5.96923076923\tTraining: 2.17295\n",
      "Step: 390\tEpoch: 5.98461538462\tTraining: 2.02286\n",
      "Step: 390\tEpoch: 6\tTraining: 0.86935\tValidation loss: 2.05805\tValidation acc: 0.45498046875\n",
      "Step: 391\tEpoch: 6.0\tTraining: 2.00831\n",
      "Step: 392\tEpoch: 6.01538461538\tTraining: 1.96413\n",
      "Step: 393\tEpoch: 6.03076923077\tTraining: 2.12922\n",
      "Step: 394\tEpoch: 6.04615384615\tTraining: 1.9241\n",
      "Step: 395\tEpoch: 6.06153846154\tTraining: 2.06599\n",
      "Step: 396\tEpoch: 6.07692307692\tTraining: 1.71635\n",
      "Step: 397\tEpoch: 6.09230769231\tTraining: 1.69658\n",
      "Step: 398\tEpoch: 6.10769230769\tTraining: 1.59432\n",
      "Step: 399\tEpoch: 6.12307692308\tTraining: 1.78242\n",
      "Step: 400\tEpoch: 6.13846153846\tTraining: 1.8023\n",
      "Step: 401\tEpoch: 6.15384615385\tTraining: 1.80853\n",
      "Step: 402\tEpoch: 6.16923076923\tTraining: 1.36678\n",
      "Step: 403\tEpoch: 6.18461538462\tTraining: 1.94244\n",
      "Step: 404\tEpoch: 6.2\tTraining: 1.40058\n",
      "Step: 405\tEpoch: 6.21538461538\tTraining: 1.92523\n",
      "Step: 406\tEpoch: 6.23076923077\tTraining: 1.37999\n",
      "Step: 407\tEpoch: 6.24615384615\tTraining: 1.96482\n",
      "Step: 408\tEpoch: 6.26153846154\tTraining: 1.76107\n",
      "Step: 409\tEpoch: 6.27692307692\tTraining: 1.22374\n",
      "Step: 410\tEpoch: 6.29230769231\tTraining: 1.57922\n",
      "Step: 411\tEpoch: 6.30769230769\tTraining: 1.29595\n",
      "Step: 412\tEpoch: 6.32307692308\tTraining: 1.46927\n",
      "Step: 413\tEpoch: 6.33846153846\tTraining: 1.40555\n",
      "Step: 414\tEpoch: 6.35384615385\tTraining: 1.15084\n",
      "Step: 415\tEpoch: 6.36923076923\tTraining: 1.21331\n",
      "Step: 416\tEpoch: 6.38461538462\tTraining: 1.03359\n",
      "Step: 417\tEpoch: 6.4\tTraining: 1.27915\n",
      "Step: 418\tEpoch: 6.41538461538\tTraining: 1.0556\n",
      "Step: 419\tEpoch: 6.43076923077\tTraining: 0.929803\n",
      "Step: 420\tEpoch: 6.44615384615\tTraining: 0.9796\n",
      "Step: 421\tEpoch: 6.46153846154\tTraining: 0.833646\n",
      "Step: 422\tEpoch: 6.47692307692\tTraining: 0.697931\n",
      "Step: 423\tEpoch: 6.49230769231\tTraining: 0.905648\n",
      "Step: 424\tEpoch: 6.50769230769\tTraining: 0.733606\n",
      "Step: 425\tEpoch: 6.52307692308\tTraining: 0.592633\n",
      "Step: 426\tEpoch: 6.53846153846\tTraining: 0.634364\n",
      "Step: 427\tEpoch: 6.55384615385\tTraining: 0.554239\n",
      "Step: 428\tEpoch: 6.56923076923\tTraining: 0.419881\n",
      "Step: 429\tEpoch: 6.58461538462\tTraining: 0.637631\n",
      "Step: 430\tEpoch: 6.6\tTraining: 0.621777\n",
      "Step: 431\tEpoch: 6.61538461538\tTraining: 0.441887\n",
      "Step: 432\tEpoch: 6.63076923077\tTraining: 0.586694\n",
      "Step: 433\tEpoch: 6.64615384615\tTraining: 0.462767\n",
      "Step: 434\tEpoch: 6.66153846154\tTraining: 0.387615\n",
      "Step: 435\tEpoch: 6.67692307692\tTraining: 0.318596\n",
      "Step: 436\tEpoch: 6.69230769231\tTraining: 0.452045\n",
      "Step: 437\tEpoch: 6.70769230769\tTraining: 0.368303\n",
      "Step: 438\tEpoch: 6.72307692308\tTraining: 0.328587\n",
      "Step: 439\tEpoch: 6.73846153846\tTraining: 0.331682\n",
      "Step: 440\tEpoch: 6.75384615385\tTraining: 0.344142\n",
      "Step: 441\tEpoch: 6.76923076923\tTraining: 0.276425\n",
      "Step: 442\tEpoch: 6.78461538462\tTraining: 0.209253\n",
      "Step: 443\tEpoch: 6.8\tTraining: 0.471911\n",
      "Step: 444\tEpoch: 6.81538461538\tTraining: 0.303747\n",
      "Step: 445\tEpoch: 6.83076923077\tTraining: 0.484002\n",
      "Step: 446\tEpoch: 6.84615384615\tTraining: 0.260457\n",
      "Step: 447\tEpoch: 6.86153846154\tTraining: 0.286415\n",
      "Step: 448\tEpoch: 6.87692307692\tTraining: 0.402741\n",
      "Step: 449\tEpoch: 6.89230769231\tTraining: 0.388105\n",
      "Step: 450\tEpoch: 6.90769230769\tTraining: 0.249749\n",
      "Step: 451\tEpoch: 6.92307692308\tTraining: 1.08113\n",
      "Step: 452\tEpoch: 6.93846153846\tTraining: 0.16035\n",
      "Step: 453\tEpoch: 6.95384615385\tTraining: 0.491163\n",
      "Step: 454\tEpoch: 6.96923076923\tTraining: 0.650512\n",
      "Step: 455\tEpoch: 6.98461538462\tTraining: 0.394457\n",
      "Step: 455\tEpoch: 7\tTraining: 0.97866\tValidation loss: 0.289354\tValidation acc: 0.9353515625\n",
      "Step: 456\tEpoch: 7.0\tTraining: 0.28766\n",
      "Step: 457\tEpoch: 7.01538461538\tTraining: 0.411243\n",
      "Step: 458\tEpoch: 7.03076923077\tTraining: 0.581938\n",
      "Step: 459\tEpoch: 7.04615384615\tTraining: 0.280391\n",
      "Step: 460\tEpoch: 7.06153846154\tTraining: 0.391974\n",
      "Step: 461\tEpoch: 7.07692307692\tTraining: 0.231942\n",
      "Step: 462\tEpoch: 7.09230769231\tTraining: 0.362249\n",
      "Step: 463\tEpoch: 7.10769230769\tTraining: 0.186747\n",
      "Step: 464\tEpoch: 7.12307692308\tTraining: 0.475195\n",
      "Step: 465\tEpoch: 7.13846153846\tTraining: 0.291633\n",
      "Step: 466\tEpoch: 7.15384615385\tTraining: 0.275705\n",
      "Step: 467\tEpoch: 7.16923076923\tTraining: 0.217275\n",
      "Step: 468\tEpoch: 7.18461538462\tTraining: 0.358111\n",
      "Step: 469\tEpoch: 7.2\tTraining: 0.246034\n",
      "Step: 470\tEpoch: 7.21538461538\tTraining: 0.215409\n",
      "Step: 471\tEpoch: 7.23076923077\tTraining: 0.213231\n",
      "Step: 472\tEpoch: 7.24615384615\tTraining: 0.189369\n",
      "Step: 473\tEpoch: 7.26153846154\tTraining: 0.200466\n",
      "Step: 474\tEpoch: 7.27692307692\tTraining: 0.110922\n",
      "Step: 475\tEpoch: 7.29230769231\tTraining: 0.175028\n",
      "Step: 476\tEpoch: 7.30769230769\tTraining: 0.256167\n",
      "Step: 477\tEpoch: 7.32307692308\tTraining: 0.170593\n",
      "Step: 478\tEpoch: 7.33846153846\tTraining: 0.138538\n",
      "Step: 479\tEpoch: 7.35384615385\tTraining: 0.113461\n",
      "Step: 480\tEpoch: 7.36923076923\tTraining: 0.121444\n",
      "Step: 481\tEpoch: 7.38461538462\tTraining: 0.16501\n",
      "Step: 482\tEpoch: 7.4\tTraining: 0.175044\n",
      "Step: 483\tEpoch: 7.41538461538\tTraining: 0.130682\n",
      "Step: 484\tEpoch: 7.43076923077\tTraining: 0.116382\n",
      "Step: 485\tEpoch: 7.44615384615\tTraining: 0.0849833\n",
      "Step: 486\tEpoch: 7.46153846154\tTraining: 0.133789\n",
      "Step: 487\tEpoch: 7.47692307692\tTraining: 0.0786008\n",
      "Step: 488\tEpoch: 7.49230769231\tTraining: 0.151757\n",
      "Step: 489\tEpoch: 7.50769230769\tTraining: 0.098875\n",
      "Step: 490\tEpoch: 7.52307692308\tTraining: 0.0774153\n",
      "Step: 491\tEpoch: 7.53846153846\tTraining: 0.0803436\n",
      "Step: 492\tEpoch: 7.55384615385\tTraining: 0.114325\n",
      "Step: 493\tEpoch: 7.56923076923\tTraining: 0.0705698\n",
      "Step: 494\tEpoch: 7.58461538462\tTraining: 0.092211\n",
      "Step: 495\tEpoch: 7.6\tTraining: 0.132051\n",
      "Step: 496\tEpoch: 7.61538461538\tTraining: 0.0779122\n",
      "Step: 497\tEpoch: 7.63076923077\tTraining: 0.0904507\n",
      "Step: 498\tEpoch: 7.64615384615\tTraining: 0.0918808\n",
      "Step: 499\tEpoch: 7.66153846154\tTraining: 0.0802089\n",
      "Step: 500\tEpoch: 7.67692307692\tTraining: 0.0588987\n",
      "Step: 501\tEpoch: 7.69230769231\tTraining: 0.0683953\n",
      "Step: 502\tEpoch: 7.70769230769\tTraining: 0.0879406\n",
      "Step: 503\tEpoch: 7.72307692308\tTraining: 0.0678775\n",
      "Step: 504\tEpoch: 7.73846153846\tTraining: 0.077761\n",
      "Step: 505\tEpoch: 7.75384615385\tTraining: 0.0814699\n",
      "Step: 506\tEpoch: 7.76923076923\tTraining: 0.0734869\n",
      "Step: 507\tEpoch: 7.78461538462\tTraining: 0.0450053\n",
      "Step: 508\tEpoch: 7.8\tTraining: 0.0782262\n",
      "Step: 509\tEpoch: 7.81538461538\tTraining: 0.0780173\n",
      "Step: 510\tEpoch: 7.83076923077\tTraining: 0.0580616\n",
      "Step: 511\tEpoch: 7.84615384615\tTraining: 0.0536695\n",
      "Step: 512\tEpoch: 7.86153846154\tTraining: 0.0759122\n",
      "Step: 513\tEpoch: 7.87692307692\tTraining: 0.0595454\n",
      "Step: 514\tEpoch: 7.89230769231\tTraining: 0.0560768\n",
      "Step: 515\tEpoch: 7.90769230769\tTraining: 0.048891\n",
      "Step: 516\tEpoch: 7.92307692308\tTraining: 0.0667509\n",
      "Step: 517\tEpoch: 7.93846153846\tTraining: 0.0416152\n",
      "Step: 518\tEpoch: 7.95384615385\tTraining: 0.040033\n",
      "Step: 519\tEpoch: 7.96923076923\tTraining: 0.0424031\n",
      "Step: 520\tEpoch: 7.98461538462\tTraining: 0.0500348\n",
      "Step: 520\tEpoch: 8\tTraining: 0.15162\tValidation loss: 0.0493149\tValidation acc: 0.9982421875\n",
      "Step: 521\tEpoch: 8.0\tTraining: 0.0340375\n",
      "Step: 522\tEpoch: 8.01538461538\tTraining: 0.0411898\n",
      "Step: 523\tEpoch: 8.03076923077\tTraining: 0.0523395\n",
      "Step: 524\tEpoch: 8.04615384615\tTraining: 0.0418105\n",
      "Step: 525\tEpoch: 8.06153846154\tTraining: 0.0656445\n",
      "Step: 526\tEpoch: 8.07692307692\tTraining: 0.0457575\n",
      "Step: 527\tEpoch: 8.09230769231\tTraining: 0.0301755\n",
      "Step: 528\tEpoch: 8.10769230769\tTraining: 0.0304367\n",
      "Step: 529\tEpoch: 8.12307692308\tTraining: 0.0411081\n",
      "Step: 530\tEpoch: 8.13846153846\tTraining: 0.0668166\n",
      "Step: 531\tEpoch: 8.15384615385\tTraining: 0.0666809\n",
      "Step: 532\tEpoch: 8.16923076923\tTraining: 0.0360493\n",
      "Step: 533\tEpoch: 8.18461538462\tTraining: 0.0616214\n",
      "Step: 534\tEpoch: 8.2\tTraining: 0.0315007\n",
      "Step: 535\tEpoch: 8.21538461538\tTraining: 0.0461274\n",
      "Step: 536\tEpoch: 8.23076923077\tTraining: 0.0387741\n",
      "Step: 537\tEpoch: 8.24615384615\tTraining: 0.0374212\n",
      "Step: 538\tEpoch: 8.26153846154\tTraining: 0.043946\n",
      "Step: 539\tEpoch: 8.27692307692\tTraining: 0.0308895\n",
      "Step: 540\tEpoch: 8.29230769231\tTraining: 0.0486129\n",
      "Step: 541\tEpoch: 8.30769230769\tTraining: 0.0524074\n",
      "Step: 542\tEpoch: 8.32307692308\tTraining: 0.0427759\n",
      "Step: 543\tEpoch: 8.33846153846\tTraining: 0.0353079\n",
      "Step: 544\tEpoch: 8.35384615385\tTraining: 0.0336421\n",
      "Step: 545\tEpoch: 8.36923076923\tTraining: 0.0286589\n",
      "Step: 546\tEpoch: 8.38461538462\tTraining: 0.0380121\n",
      "Step: 547\tEpoch: 8.4\tTraining: 0.047988\n",
      "Step: 548\tEpoch: 8.41538461538\tTraining: 0.0360511\n",
      "Step: 549\tEpoch: 8.43076923077\tTraining: 0.0295463\n",
      "Step: 550\tEpoch: 8.44615384615\tTraining: 0.0271796\n",
      "Step: 551\tEpoch: 8.46153846154\tTraining: 0.0408032\n",
      "Step: 552\tEpoch: 8.47692307692\tTraining: 0.0242466\n",
      "Step: 553\tEpoch: 8.49230769231\tTraining: 0.0343161\n",
      "Step: 554\tEpoch: 8.50769230769\tTraining: 0.0311717\n",
      "Step: 555\tEpoch: 8.52307692308\tTraining: 0.0313371\n",
      "Step: 556\tEpoch: 8.53846153846\tTraining: 0.0239452\n",
      "Step: 557\tEpoch: 8.55384615385\tTraining: 0.0293585\n",
      "Step: 558\tEpoch: 8.56923076923\tTraining: 0.0276422\n",
      "Step: 559\tEpoch: 8.58461538462\tTraining: 0.0302488\n",
      "Step: 560\tEpoch: 8.6\tTraining: 0.0518427\n",
      "Step: 561\tEpoch: 8.61538461538\tTraining: 0.0370091\n",
      "Step: 562\tEpoch: 8.63076923077\tTraining: 0.0325686\n",
      "Step: 563\tEpoch: 8.64615384615\tTraining: 0.0350283\n",
      "Step: 564\tEpoch: 8.66153846154\tTraining: 0.0349948\n",
      "Step: 565\tEpoch: 8.67692307692\tTraining: 0.0240437\n",
      "Step: 566\tEpoch: 8.69230769231\tTraining: 0.026915\n",
      "Step: 567\tEpoch: 8.70769230769\tTraining: 0.0353729\n",
      "Step: 568\tEpoch: 8.72307692308\tTraining: 0.0286292\n",
      "Step: 569\tEpoch: 8.73846153846\tTraining: 0.0361552\n",
      "Step: 570\tEpoch: 8.75384615385\tTraining: 0.0396079\n",
      "Step: 571\tEpoch: 8.76923076923\tTraining: 0.027678\n",
      "Step: 572\tEpoch: 8.78461538462\tTraining: 0.0215196\n",
      "Step: 573\tEpoch: 8.8\tTraining: 0.0385557\n",
      "Step: 574\tEpoch: 8.81538461538\tTraining: 0.0356833\n",
      "Step: 575\tEpoch: 8.83076923077\tTraining: 0.0246738\n",
      "Step: 576\tEpoch: 8.84615384615\tTraining: 0.0245237\n",
      "Step: 577\tEpoch: 8.86153846154\tTraining: 0.0347967\n",
      "Step: 578\tEpoch: 8.87692307692\tTraining: 0.0287539\n",
      "Step: 579\tEpoch: 8.89230769231\tTraining: 0.0237124\n",
      "Step: 580\tEpoch: 8.90769230769\tTraining: 0.0235822\n",
      "Step: 581\tEpoch: 8.92307692308\tTraining: 0.0306875\n",
      "Step: 582\tEpoch: 8.93846153846\tTraining: 0.0203407\n",
      "Step: 583\tEpoch: 8.95384615385\tTraining: 0.0210607\n",
      "Step: 584\tEpoch: 8.96923076923\tTraining: 0.0220002\n",
      "Step: 585\tEpoch: 8.98461538462\tTraining: 0.0250589\n",
      "Step: 585\tEpoch: 9\tTraining: 0.035698\tValidation loss: 0.0257854\tValidation acc: 0.9994140625\n",
      "Step: 586\tEpoch: 9.0\tTraining: 0.0173405\n",
      "Step: 587\tEpoch: 9.01538461538\tTraining: 0.0210261\n",
      "Step: 588\tEpoch: 9.03076923077\tTraining: 0.0216216\n",
      "Step: 589\tEpoch: 9.04615384615\tTraining: 0.0209318\n",
      "Step: 590\tEpoch: 9.06153846154\tTraining: 0.0329989\n",
      "Step: 591\tEpoch: 9.07692307692\tTraining: 0.0249593\n",
      "Step: 592\tEpoch: 9.09230769231\tTraining: 0.0155905\n",
      "Step: 593\tEpoch: 9.10769230769\tTraining: 0.0154833\n",
      "Step: 594\tEpoch: 9.12307692308\tTraining: 0.0207005\n",
      "Step: 595\tEpoch: 9.13846153846\tTraining: 0.03547\n",
      "Step: 596\tEpoch: 9.15384615385\tTraining: 0.0319481\n",
      "Step: 597\tEpoch: 9.16923076923\tTraining: 0.0195231\n",
      "Step: 598\tEpoch: 9.18461538462\tTraining: 0.0311938\n",
      "Step: 599\tEpoch: 9.2\tTraining: 0.0173605\n",
      "Step: 600\tEpoch: 9.21538461538\tTraining: 0.0248804\n",
      "Step: 601\tEpoch: 9.23076923077\tTraining: 0.0208239\n",
      "Step: 602\tEpoch: 9.24615384615\tTraining: 0.0206189\n",
      "Step: 603\tEpoch: 9.26153846154\tTraining: 0.0250533\n",
      "Step: 604\tEpoch: 9.27692307692\tTraining: 0.0178762\n",
      "Step: 605\tEpoch: 9.29230769231\tTraining: 0.0263924\n",
      "Step: 606\tEpoch: 9.30769230769\tTraining: 0.0291296\n",
      "Step: 607\tEpoch: 9.32307692308\tTraining: 0.020778\n",
      "Step: 608\tEpoch: 9.33846153846\tTraining: 0.0192609\n",
      "Step: 609\tEpoch: 9.35384615385\tTraining: 0.0187078\n",
      "Step: 610\tEpoch: 9.36923076923\tTraining: 0.0157264\n",
      "Step: 611\tEpoch: 9.38461538462\tTraining: 0.0211699\n",
      "Step: 612\tEpoch: 9.4\tTraining: 0.0267088\n",
      "Step: 613\tEpoch: 9.41538461538\tTraining: 0.0198106\n",
      "Step: 614\tEpoch: 9.43076923077\tTraining: 0.0170401\n",
      "Step: 615\tEpoch: 9.44615384615\tTraining: 0.0166249\n",
      "Step: 616\tEpoch: 9.46153846154\tTraining: 0.0245169\n",
      "Step: 617\tEpoch: 9.47692307692\tTraining: 0.0138841\n",
      "Step: 618\tEpoch: 9.49230769231\tTraining: 0.0204523\n",
      "Step: 619\tEpoch: 9.50769230769\tTraining: 0.0189745\n",
      "Step: 620\tEpoch: 9.52307692308\tTraining: 0.0189901\n",
      "Step: 621\tEpoch: 9.53846153846\tTraining: 0.014524\n",
      "Step: 622\tEpoch: 9.55384615385\tTraining: 0.0165601\n",
      "Step: 623\tEpoch: 9.56923076923\tTraining: 0.0161561\n",
      "Step: 624\tEpoch: 9.58461538462\tTraining: 0.0179611\n",
      "Step: 625\tEpoch: 9.6\tTraining: 0.0294552\n",
      "Step: 626\tEpoch: 9.61538461538\tTraining: 0.0239317\n",
      "Step: 627\tEpoch: 9.63076923077\tTraining: 0.0202179\n",
      "Step: 628\tEpoch: 9.64615384615\tTraining: 0.0218089\n",
      "Step: 629\tEpoch: 9.66153846154\tTraining: 0.0185131\n",
      "Step: 630\tEpoch: 9.67692307692\tTraining: 0.0144084\n",
      "Step: 631\tEpoch: 9.69230769231\tTraining: 0.0171285\n",
      "Step: 632\tEpoch: 9.70769230769\tTraining: 0.0239789\n",
      "Step: 633\tEpoch: 9.72307692308\tTraining: 0.0175814\n",
      "Step: 634\tEpoch: 9.73846153846\tTraining: 0.0205885\n",
      "Step: 635\tEpoch: 9.75384615385\tTraining: 0.0216142\n",
      "Step: 636\tEpoch: 9.76923076923\tTraining: 0.0162552\n",
      "Step: 637\tEpoch: 9.78461538462\tTraining: 0.0129619\n",
      "Step: 638\tEpoch: 9.8\tTraining: 0.0226579\n",
      "Step: 639\tEpoch: 9.81538461538\tTraining: 0.0237979\n",
      "Step: 640\tEpoch: 9.83076923077\tTraining: 0.0159472\n",
      "Step: 641\tEpoch: 9.84615384615\tTraining: 0.0150768\n",
      "Step: 642\tEpoch: 9.86153846154\tTraining: 0.0219317\n",
      "Step: 643\tEpoch: 9.87692307692\tTraining: 0.0180989\n",
      "Step: 644\tEpoch: 9.89230769231\tTraining: 0.0158142\n",
      "Step: 645\tEpoch: 9.90769230769\tTraining: 0.0150153\n",
      "Step: 646\tEpoch: 9.92307692308\tTraining: 0.0197985\n",
      "Step: 647\tEpoch: 9.93846153846\tTraining: 0.0132885\n",
      "Step: 648\tEpoch: 9.95384615385\tTraining: 0.0121046\n",
      "Step: 649\tEpoch: 9.96923076923\tTraining: 0.0133109\n",
      "Step: 650\tEpoch: 9.98461538462\tTraining: 0.016607\n",
      "Step: 650\tEpoch: 10\tTraining: 0.020164\tValidation loss: 0.0172676\tValidation acc: 0.99970703125\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "for t in range(epochs):\n",
    "    # Training\n",
    "    train_loss = []\n",
    "    \n",
    "    for i in range(num_train/batch_size):\n",
    "        train_loss.append(train_batch(i))\n",
    "        step += 1\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            print \"Step: \" + str(step) + \"\\tEpoch: \" + str(t + resume_at + float(i)/(num_train/batch_size)) + \"\\tTraining: \" + str(train_loss[-1])\n",
    "            \n",
    "        if step % 200 == 0:\n",
    "            saver.save(sess, 'checkpoints/saved-model-1off-attn', global_step=step)\n",
    "        \n",
    "    train_loss = np.mean(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    valid_loss = []\n",
    "    valid_acc = []\n",
    "    \n",
    "    for i in range(num_validation/batch_size):\n",
    "        this_loss, this_acc = validate_batch(i)\n",
    "        valid_loss.append(this_loss)\n",
    "        valid_acc.append(this_acc)\n",
    "        #print this_acc\n",
    "        \n",
    "    valid_loss = np.mean(valid_loss)\n",
    "    valid_acc = np.mean(valid_acc)\n",
    "    \n",
    "    print \"Step: \" + str(step) + \"\\tEpoch: \" + str(t + resume_at + 1) + \"\\tTraining: \" + str(train_loss) + \"\\tValidation loss: \" + str(valid_loss) + \"\\tValidation acc: \" + str(valid_acc)\n",
    "    saver.save(sess, 'checkpoints/saved-model-1off-attn', global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
